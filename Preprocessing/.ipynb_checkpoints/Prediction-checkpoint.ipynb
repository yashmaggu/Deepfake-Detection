{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68fb276-d3ef-465b-bc80-902300d77258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #USE CUDA if gpu is represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f04148-fc18-4c3f-80f0-8a029bf542b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained = True)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048,num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self, x):\n",
    "        batch_size,seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size,seq_length,2048)\n",
    "        x_lstm,_ = self.lstm(x,None)\n",
    "        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8122a8-7278-4d9a-9146-6694315d3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction Function\n",
    "im_size = 112\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "sm = nn.Softmax()\n",
    "inv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.squeeze()\n",
    "    image = inv_normalize(image)\n",
    "    image = image.numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image.clip(0, 1)\n",
    "    #cv2.imwrite('./2.png',image*255)\n",
    "    return image\n",
    "\n",
    "def predict(model,img,path = './'):\n",
    "  fmap,logits = model(img.to('cuda'))\n",
    "  params = list(model.parameters())\n",
    "  weight_softmax = model.linear1.weight.detach().cpu().numpy()\n",
    "  logits = sm(logits)\n",
    "  _,prediction = torch.max(logits,1)\n",
    "  confidence = logits[:,int(prediction.item())].item()*100\n",
    " \n",
    "  idx = np.argmax(logits.detach().cpu().numpy())\n",
    "  bz, nc, h, w = fmap.shape\n",
    "  out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n",
    "  predict = out.reshape(h,w)\n",
    "  predict = predict - np.min(predict)\n",
    "  predict_img = predict / np.max(predict)\n",
    "  predict_img = np.uint8(255*predict_img)\n",
    "  out = cv2.resize(predict_img, (im_size,im_size))\n",
    "  heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)\n",
    "  img = im_convert(img[:,-1,:,:,:])\n",
    "  result = heatmap * 0.5 + img*0.8*255\n",
    "  #cv2.imwrite('/content/1.png',result)\n",
    "  result1 = heatmap * 0.5/255 + img*0.8\n",
    "  r,g,b = cv2.split(result1)\n",
    "  result1 = cv2.merge((r,g,b))\n",
    "  plt.imshow(result1)\n",
    "  plt.show()\n",
    "  print('confidence of prediction:',logits[:,int(prediction.item())].item()*100)\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  return [int(prediction.item()),confidence]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a1f521-a924-430a-9f14-7bb136ea5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "#Output confusion matrix\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('True positive = ', cm[0][0])\n",
    "    print('False positive = ', cm[0][1])\n",
    "    print('False negative = ', cm[1][0])\n",
    "    print('True negative = ', cm[1][1])\n",
    "    print('\\n')\n",
    "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "    plt.ylabel('Actual label', size = 20)\n",
    "    plt.xlabel('Predicted label', size = 20)\n",
    "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "    plt.ylim([2, 0])\n",
    "    plt.show()\n",
    "    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n",
    "    print(\"Calculated Accuracy\",calculated_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac1b1a3-7d70-4fc3-9317-aec8740c61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_face_video(Dataset): # extracts face from given video for prediction\n",
    "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
    "        self.video_names = video_names\n",
    "        self.transform = transform\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    def __getitem__(self,idx):\n",
    "        video_path = self.video_names[idx]\n",
    "        frames = []\n",
    "        a = int(100/self.count)\n",
    "        first_frame = np.random.randint(0,a)\n",
    "        face_detector = MTCNN(device=device) #MTCNN for face detection\n",
    "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
    "         \n",
    "            try:\n",
    "                face = face_detector.detect(frame)[0][0]\n",
    "            except:\n",
    "                continue\n",
    "            try:              \n",
    "                left,bottom,right,top=face\n",
    "                left=int(left)\n",
    "                bottom=int(bottom)\n",
    "                right=int(right)\n",
    "                top=int(top)\n",
    "                frame=frame[bottom:top,left:right] \n",
    "            except:\n",
    "                pass\n",
    "           \n",
    "            frames.append(self.transform(frame))\n",
    "            if(len(frames) == self.count):\n",
    "              break\n",
    "        #print(\"no of frames\",len(frames))\n",
    "        frames = torch.stack(frames)\n",
    "        frames = frames[:self.count]\n",
    "        return frames.unsqueeze(0)\n",
    "    def frame_extract(self,path):\n",
    "      vidObj = cv2.VideoCapture(path)\n",
    "      success = 1\n",
    "      while success:\n",
    "          success, image = vidObj.read()\n",
    "          if success:\n",
    "              yield image\n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
    "    b,g,r = cv2.split(image)\n",
    "    image = cv2.merge((r,g,b))\n",
    "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
    "    image = image*255.0\n",
    "    plt.imshow(image.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01ca1a7-9ea7-49b3-880f-e6786429dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Predict_Test\\id_dhruvrathee.mp4\n",
      "img\n",
      "tensor([[[[[-1.1075, -0.9705, -0.9192,  ..., -1.7412, -1.6898, -1.6727],\n",
      "           [-1.1075, -0.9020, -0.8678,  ..., -1.7069, -1.6727, -1.6555],\n",
      "           [-1.0733, -0.8507, -0.7993,  ..., -1.6727, -1.6384, -1.6384],\n",
      "           ...,\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -1.9980, -1.9980, -1.9980],\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -2.0152, -2.0152, -2.0152],\n",
      "           [-1.6555, -1.6555, -1.6727,  ..., -2.0837, -2.0837, -2.0837]],\n",
      "\n",
      "          [[-1.0553, -0.9328, -0.8627,  ..., -1.5805, -1.5630, -1.5630],\n",
      "           [-1.0203, -0.7927, -0.7577,  ..., -1.5105, -1.5105, -1.5455],\n",
      "           [-0.9853, -0.7752, -0.7402,  ..., -1.4580, -1.4755, -1.5105],\n",
      "           ...,\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -1.9832, -1.9832],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -1.9832, -1.9832],\n",
      "           [-1.8782, -1.8782, -1.8957,  ..., -2.0182, -2.0007, -2.0182]],\n",
      "\n",
      "          [[-0.7238, -0.5844, -0.5147,  ..., -1.0724, -1.1247, -1.1596],\n",
      "           [-0.6715, -0.4275, -0.3753,  ..., -0.9678, -1.0550, -1.1421],\n",
      "           [-0.6193, -0.3753, -0.2532,  ..., -0.8633, -0.9504, -1.0376],\n",
      "           ...,\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.8044, -1.8044],\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7870, -1.7870],\n",
      "           [-1.7870, -1.7870, -1.7696,  ..., -1.5953, -1.6302, -1.6476]]],\n",
      "\n",
      "\n",
      "         [[[-1.1075, -0.9877, -0.9192,  ..., -1.7240, -1.6898, -1.6727],\n",
      "           [-1.1247, -0.9192, -0.8678,  ..., -1.6898, -1.6555, -1.6555],\n",
      "           [-1.0904, -0.8678, -0.7993,  ..., -1.6555, -1.6384, -1.6384],\n",
      "           ...,\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -1.9980, -1.9980, -1.9980],\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -2.0323, -2.0152, -2.0323],\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "          [[-1.0728, -0.9328, -0.8803,  ..., -1.5630, -1.5630, -1.5630],\n",
      "           [-1.0378, -0.8102, -0.7752,  ..., -1.4930, -1.4930, -1.5455],\n",
      "           [-1.0028, -0.7752, -0.7402,  ..., -1.4405, -1.4580, -1.4930],\n",
      "           ...,\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -2.0007, -1.9832],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -2.0007, -1.9832],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "          [[-0.7238, -0.5844, -0.5321,  ..., -1.0724, -1.1073, -1.1596],\n",
      "           [-0.6890, -0.4624, -0.3927,  ..., -0.9504, -1.0376, -1.1247],\n",
      "           [-0.6367, -0.3927, -0.2707,  ..., -0.8458, -0.9504, -1.0376],\n",
      "           ...,\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7522, -1.7696],\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.6302, -1.6127, -1.6476]]],\n",
      "\n",
      "\n",
      "         [[[-1.1932, -1.0904, -0.9705,  ..., -1.7583, -1.7240, -1.6727],\n",
      "           [-1.2788, -1.1418, -0.9363,  ..., -1.7412, -1.7069, -1.6555],\n",
      "           [-1.2788, -1.1247, -0.8678,  ..., -1.6727, -1.6555, -1.6384],\n",
      "           ...,\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -1.9980, -1.9980, -1.9980],\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -1.9980, -1.9980, -1.9980],\n",
      "           [-1.6555, -1.6555, -1.6555,  ..., -2.0323, -2.0152, -2.0152]],\n",
      "\n",
      "          [[-1.1954, -1.0903, -0.9853,  ..., -1.5805, -1.5630, -1.5630],\n",
      "           [-1.2129, -1.0728, -0.8627,  ..., -1.5105, -1.5280, -1.5105],\n",
      "           [-1.1779, -1.0203, -0.7752,  ..., -1.4055, -1.4755, -1.4755],\n",
      "           ...,\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -1.9832, -1.9832, -1.9832],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -2.0007, -2.0007],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -2.0007, -2.0007, -2.0007]],\n",
      "\n",
      "          [[-0.8633, -0.7587, -0.6367,  ..., -1.0550, -1.1073, -1.1421],\n",
      "           [-0.8458, -0.7238, -0.4973,  ..., -0.9156, -1.0027, -1.0724],\n",
      "           [-0.8284, -0.6715, -0.3927,  ..., -0.7413, -0.8807, -0.9678],\n",
      "           ...,\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7696, -1.7870, -1.7870],\n",
      "           [-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7696, -1.7696]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.9877, -1.0219, -1.0219,  ..., -1.6384, -1.6384, -1.6727],\n",
      "           [-0.9877, -0.9534, -0.9534,  ..., -1.5357, -1.5528, -1.6384],\n",
      "           [-0.9877, -0.9363, -0.9363,  ..., -1.3815, -1.4500, -1.5357],\n",
      "           ...,\n",
      "           [-1.6898, -1.6898, -1.6898,  ..., -0.9020, -0.8678, -0.7822],\n",
      "           [-1.7069, -1.7069, -1.7069,  ..., -0.9020, -0.8678, -0.7993],\n",
      "           [-1.7240, -1.7240, -1.7240,  ..., -0.9020, -0.9020, -0.8507]],\n",
      "\n",
      "          [[-1.0378, -1.0553, -1.0203,  ..., -1.4055, -1.4580, -1.5105],\n",
      "           [-1.0028, -0.9503, -0.9503,  ..., -1.2829, -1.3880, -1.4930],\n",
      "           [-0.8978, -0.8627, -0.8978,  ..., -1.1253, -1.2654, -1.3880],\n",
      "           ...,\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.1800, -0.1625, -0.1099],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.1975, -0.1975, -0.1450],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.2325, -0.2150, -0.1975]],\n",
      "\n",
      "          [[-0.7238, -0.7413, -0.7064,  ..., -0.7238, -0.8458, -0.9330],\n",
      "           [-0.6541, -0.6018, -0.5844,  ..., -0.5495, -0.7238, -0.8633],\n",
      "           [-0.5147, -0.4624, -0.4450,  ..., -0.3404, -0.5670, -0.7238],\n",
      "           ...,\n",
      "           [-1.7870, -1.7870, -1.7870,  ...,  1.2457,  1.2282,  1.2108],\n",
      "           [-1.7696, -1.7696, -1.7696,  ...,  1.2631,  1.2457,  1.1934],\n",
      "           [-1.7696, -1.7696, -1.7696,  ...,  1.2631,  1.2457,  1.1759]]],\n",
      "\n",
      "\n",
      "         [[[-1.0562, -1.0904, -1.1075,  ..., -1.6555, -1.6555, -1.6898],\n",
      "           [-0.9877, -0.9877, -1.0048,  ..., -1.6213, -1.6555, -1.6898],\n",
      "           [-1.0562, -0.9877, -0.9534,  ..., -1.5357, -1.6042, -1.7069],\n",
      "           ...,\n",
      "           [-1.7240, -1.7240, -1.7240,  ..., -0.9020, -0.7993, -0.6794],\n",
      "           [-1.7240, -1.7240, -1.7240,  ..., -0.9020, -0.8507, -0.7308],\n",
      "           [-1.7240, -1.7240, -1.7240,  ..., -0.9020, -0.8678, -0.7993]],\n",
      "\n",
      "          [[-1.1078, -1.1429, -1.1429,  ..., -1.4930, -1.5105, -1.5630],\n",
      "           [-1.0203, -1.0378, -1.0378,  ..., -1.4405, -1.5105, -1.5455],\n",
      "           [-1.0553, -1.0028, -0.9503,  ..., -1.3354, -1.4580, -1.5630],\n",
      "           ...,\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.2150, -0.1625, -0.1099],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.1975, -0.1975, -0.1625],\n",
      "           [-1.8782, -1.8782, -1.8782,  ..., -0.1975, -0.1975, -0.1800]],\n",
      "\n",
      "          [[-0.8110, -0.8458, -0.8458,  ..., -0.9156, -1.0201, -1.1073],\n",
      "           [-0.7238, -0.7238, -0.7413,  ..., -0.7936, -0.8981, -1.0027],\n",
      "           [-0.7064, -0.6367, -0.6018,  ..., -0.6715, -0.8284, -0.9504],\n",
      "           ...,\n",
      "           [-1.7696, -1.7696, -1.7696,  ...,  1.2457,  1.1934,  1.0539],\n",
      "           [-1.7696, -1.7696, -1.7696,  ...,  1.2457,  1.1759,  1.0714],\n",
      "           [-1.7696, -1.7696, -1.7696,  ...,  1.2980,  1.2282,  1.1237]]],\n",
      "\n",
      "\n",
      "         [[[-1.0733, -1.0733, -1.1075,  ..., -1.6384, -1.6555, -1.6898],\n",
      "           [-1.0562, -1.0048, -1.0048,  ..., -1.6384, -1.6555, -1.7069],\n",
      "           [-1.1589, -1.0562, -0.9877,  ..., -1.5870, -1.6727, -1.7240],\n",
      "           ...,\n",
      "           [-0.6965, -0.6452, -0.6623,  ..., -0.8678, -0.7650, -0.6452],\n",
      "           [-0.3027, -0.2171, -0.1828,  ..., -0.8849, -0.8164, -0.7308],\n",
      "           [-0.1999, -0.1828, -0.1486,  ..., -0.8849, -0.8507, -0.7993]],\n",
      "\n",
      "          [[-1.1253, -1.1253, -1.1604,  ..., -1.4930, -1.5455, -1.5805],\n",
      "           [-1.0903, -1.0553, -1.0553,  ..., -1.4755, -1.5280, -1.5630],\n",
      "           [-1.1779, -1.0728, -1.0028,  ..., -1.4230, -1.5280, -1.5805],\n",
      "           ...,\n",
      "           [-0.8803, -0.8102, -0.8102,  ..., -0.1975, -0.1625, -0.1275],\n",
      "           [-0.3725, -0.2850, -0.2675,  ..., -0.2150, -0.1975, -0.1625],\n",
      "           [-0.2675, -0.2500, -0.2150,  ..., -0.1975, -0.1975, -0.1800]],\n",
      "\n",
      "          [[-0.8284, -0.8284, -0.8633,  ..., -1.0201, -1.1073, -1.1596],\n",
      "           [-0.7936, -0.7413, -0.7587,  ..., -0.8981, -0.9853, -1.0724],\n",
      "           [-0.8284, -0.7413, -0.6715,  ..., -0.7936, -0.9330, -1.0376],\n",
      "           ...,\n",
      "           [-0.5670, -0.5321, -0.5495,  ...,  1.1934,  1.0888,  0.9319],\n",
      "           [-0.0790,  0.0082,  0.0256,  ...,  1.2457,  1.1411,  1.0191],\n",
      "           [ 0.0256,  0.0431,  0.0779,  ...,  1.2805,  1.2108,  1.1237]]]]])\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m trueLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trueLabel\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(trueLabel)\n\u001b[1;32m---> 35\u001b[0m pred\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trueLabel \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     37\u001b[0m     true\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "#Code for making prediction\n",
    "im_size = 112\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "\n",
    "\n",
    "path_to_videos= glob.glob(\"./Predict_Test/*.mp4\")\n",
    "print(path_to_videos)\n",
    "# path_to_videos= glob.glob(\"../Dataset/YouTube-real/*.mp4\")\n",
    "video_dataset = extract_face_video(path_to_videos,sequence_length = 20,transform = train_transforms) #Predict based on first 20 frames\n",
    "\n",
    "model = Model(2).cuda()\n",
    "path_to_model = './trained-model-small.pt'\n",
    "# path_to_model = './trained-model.pt'\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "true=[]\n",
    "pred=[]\n",
    "with torch.no_grad():\n",
    "    for i in range(0,len(path_to_videos)):\n",
    "      print(path_to_videos[i])\n",
    "      prediction = predict(model,video_dataset[i],'./')\n",
    "      gc.collect() #Avoid memory leak\n",
    "      \n",
    "      #For confusion matrix\n",
    "      trueLabel=path_to_videos[i].split(\"\\\\\")[-1]\n",
    "      trueLabel=len(trueLabel.split(\"_\"))\n",
    "      print(trueLabel)\n",
    "      pred.append(prediction[0])\n",
    "      if trueLabel <= 2:\n",
    "          true.append(1)\n",
    "      elif trueLabel == 3:\n",
    "          true.append(0)\n",
    "    \n",
    "      if prediction[0] == 1:      \n",
    "        print(\"REAL\")\n",
    "      else:\n",
    "        print(\"FAKE\")\n",
    "\n",
    "print_confusion_matrix(true,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0d324-6c21-45c3-9c88-a14598ee873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
